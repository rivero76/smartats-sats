This feature is the ultimate onboarding cheat code. By removing the friction of manual data entry, you dramatically increase the likelihood that a user actually completes their profile and experiences the "aha!" moment of your ATS matching engine.

Since this process involves taking unstructured data from a third-party source, mapping it via an LLM, and merging it with potential existing data without silently overwriting it, the prompt needs to explicitly instruct Codex on how to handle the data merge safely.

Here is the structured Markdown prompt. Copy everything between the lines and paste it directly into your VS Code AI chat:

Role: Act as a Senior Full-Stack Lead Developer.

Context: We are executing Phase P13 of our product roadmap: "LinkedIn Profile Ingestion Loop." Currently, our app only stores a user's LinkedIn URL as a string (Settings.tsx:220, useProfile.ts:175). We need to build the functionality to fetch that profile data, parse it using an LLM, and structure it into our existing Supabase tables (sats_user_skills and sats_skill_experiences). Crucially, we must maintain our application's "Human-in-the-Loop" (HITL) trust model by allowing the user to review and approve the imported data before it is permanently saved, and we must deduplicate against any existing resume-derived data. Our stack is React, Supabase (PostgreSQL), and Supabase Edge Functions.

Task: Please review the following Agile User Stories. For each story, generate the necessary Supabase database migrations, Edge Function code (TypeScript), and React UI components to satisfy the Acceptance Criteria. Let's tackle them one by one.

Epic: LinkedIn Profile Ingestion & Structuring
User Story 1: The Ingestion Edge Function & LLM Parser

As the backend system,

I need an endpoint to fetch a user's LinkedIn profile data and use an LLM to map it to our internal schema,

So that unstructured social profile data becomes queryable career data.

Acceptance Criteria:

Create a Supabase Edge Function (linkedin-profile-ingest).

Mock API Integration: Assume the raw profile data is fetched from a third-party API (e.g., Proxycurl) returning a JSON object with experiences and skills arrays. Mock this payload for now.

Pass the mock payload to our LLM using a strict JSON schema (adhering to our Phase P10 structured output standards) to normalize the data into shapes compatible with sats_user_skills and sats_skill_experiences.

The Edge Function should return the structured data back to the client without inserting it into the database yet (to support HITL review).

User Story 2: Identity Merge & Deduplication Logic

As a database architect,

I need to ensure that importing LinkedIn data doesn't create duplicate skills or conflicting timelines if the user has already uploaded a resume,

So that the user's career baseline remains accurate and clean.

Acceptance Criteria:

Implement a deterministic frontend merge utility (`src/utils/linkedinImportMerge.ts`) that takes:
- proposed LinkedIn preview rows (skills + skill experiences), and
- existing user baseline rows from `sats_user_skills` and `sats_skill_experiences`.

For skills, apply canonicalization + fuzzy matching:
- normalize case/punctuation/spacing,
- alias obvious variants (e.g., `react.js` -> `react`, `node.js` -> `node`),
- exact canonical match => `merge`,
- high similarity (Dice/Jaro style threshold, e.g. `>= 0.86`) => `merge`,
- otherwise => `insert`.

For experiences, apply duplicate fingerprinting to avoid timeline collisions:
- fingerprint fields: canonical skill + normalized job title + company + truncated normalized description.
- exact fingerprint match => `ignore`.

The utility must produce explicit decision buckets:
- `skills_to_insert`
- `skills_to_merge`
- `skills_ignored`
- `experiences_to_insert`
- `experiences_ignored`

Every prepared row must include provenance metadata:
- `source: 'linkedin'`
- `import_date: <ISO timestamp>`

Do not silently overwrite existing rows in Story 2:
- mark potential updates as `merge` candidates for user review in HITL step.

Important schema requirement for this codebase:
- because `sats_user_skills` and `sats_skill_experiences` require `skill_id`, include a skill resolution plan:
  - map proposed `skill_name` to existing `sats_skills.id` when found,
  - create missing `sats_skills` entries in save flow (Story 3) before inserting dependent rows.

Add unit tests covering:
- `React` vs `React.js` merge behavior,
- duplicate experience detection,
- provenance tagging on all insert-ready objects.

User Story 3: Human-in-the-Loop (HITL) Review UI

As a user,

I want to see exactly what skills and experiences were pulled from my LinkedIn and choose what to keep,

So that I have total control over my profile and can correct any AI mapping mistakes.

Acceptance Criteria:

Update Settings.tsx to include an "Import Profile" button next to the LinkedIn URL input.

Create a new React modal component (ProfileImportReviewModal.tsx).

Display the parsed experiences and skills side-by-side. Include a checkbox next to each item (defaulted to checked).

When the user clicks "Approve and Save," execute a Supabase RPC or standard insert/upsert batch operation to save only the selected items into sats_user_skills and sats_skill_experiences.

Handle loading states, error boundaries, and success toast notifications.

PM Strategy Note
For User Story 2 (Deduplication), do not rely on exact-string matching only.
Use canonicalization + fuzzy thresholds so `"React"` and `"React.js"` are treated as the same skill class while still avoiding aggressive false merges.
