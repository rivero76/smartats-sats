Role: Act as a Senior Full-Stack Lead Developer.

Context: We are executing Phase P14 of our product roadmap: "The Proactive Search Engine." Today, SmartATS is primarily a reactive ATS matcher (user provides JD -> system scores resume/JD). P14 introduces proactive discovery and async scoring. Stack: React, Supabase (PostgreSQL), Supabase Edge Functions.

Execution rule: Implement user stories one by one. Do not start Story N+1 until Story N is complete and verified.

Codebase conventions (must follow):
1. Domain tables use `sats_` prefix (`sats_analyses`, `sats_job_descriptions`, etc.).
2. Columns are `snake_case`.
3. Edge Functions use kebab-case names.
4. Reuse existing ATS schema/flows where possible (`sats_analyses`, ATS output shape, missing skills arrays, score fields).

Critical architecture decisions for P14:
1. Baseline strategy for proactive scoring:
- Primary baseline: user profile skills (`sats_user_skills` + `sats_skill_experiences`).
- Fallback baseline (when skills sparse): most recent active resume extraction text.
2. Async orchestration strategy:
- Use queue-style processing via scheduled scorer (`async-ats-scorer`) instead of row-level trigger fanout.
3. Threshold strategy:
- Effective threshold = `profiles.proactive_match_threshold` (if set) else global default (0.60).

Epic: Proactive Search and Scoring Engine

User Story 1: Background Job Fetcher (Staging)
As the backend system,
I need a scheduled worker to fetch new job listings from external sources and stage them in our database,
So that the scoring engine has a fresh pool of roles to evaluate without user real-time waits.

Acceptance Criteria:
1. Create table `sats_staged_jobs` for external jobs:
- Required fields: `id`, `source`, `source_url`, `title`, `company_name`, `description_raw`, `description_normalized`, `content_hash`, `fetched_at`, `status`, `error_message`, `created_at`, `updated_at`.
- Deduplication: unique on `source_url`; secondary dedupe by `content_hash`.
2. Add index strategy:
- `idx_sats_staged_jobs_status_created_at`
- `idx_sats_staged_jobs_source_fetched_at`
3. Build Edge Function `fetch-market-jobs`:
- Triggered by `pg_cron`.
- For initial implementation, ingest a mock JSON array.
- Upsert into `sats_staged_jobs` with dedupe behavior.
4. Add scheduler migration:
- Cron runs `fetch-market-jobs` on a defined interval.
- Log fetch run outcomes via existing centralized logging.

Definition of Done (Story 1):
1. Scheduled function inserts/updates staged rows without duplicates.
2. Rows are marked with operational status (`queued`, `processed`, `error`).
3. Run is observable in logs.

User Story 2: Async ATS Scoring Pipeline
As the matching engine,
I need to automatically score each new staged job against active user baselines,
So that adherence can be calculated without manual JD creation.

Acceptance Criteria:
1. Build Edge Function `async-ats-scorer` (scheduled worker pattern):
- Pull `sats_staged_jobs` with `status='queued'`.
- Build candidate baseline from `sats_user_skills`/`sats_skill_experiences` (fallback to latest resume extraction when needed).
2. Reuse ATS logic/prompt contract:
- Keep ATS output compatible with current schema expectations (`match_score`, matched/missing skills, recommendations, score breakdown).
3. Persist scoring in existing ATS tracking:
- Materialize staged job into `sats_job_descriptions` (system-generated source record).
- Create/update linked `sats_analyses` row per `(user_id, staged_job_id)` equivalent key.
- Store proactive metadata in `analysis_data` (source, staged_job_id, baseline_type).
4. Update staged row status after scoring:
- `processed` on success; `error` with reason on failure.

Definition of Done (Story 2):
1. New staged jobs are scored asynchronously for eligible users.
2. ATS-compatible records are queryable from existing analysis tables.
3. Pipeline is idempotent and retry-safe.

User Story 3: Threshold Filtering and Notification Engine
As a job applicant,
I want notifications only for jobs above my match threshold,
So I focus on high-probability opportunities.

Acceptance Criteria:
1. Create `sats_user_notifications` table:
- Fields: `id`, `user_id`, `type`, `title`, `message`, `payload`, `is_read`, `read_at`, `created_at`.
- Payload includes `staged_job_id`, `analysis_id`, `match_score`, `threshold_used`.
2. Add threshold logic to `async-ats-scorer`:
- If `match_score >= effective_threshold`, insert notification.
- Prevent duplicates with unique guard on `(user_id, type, payload->staged_job_id)` via normalized key column if needed.
3. Threshold configuration:
- Add `profiles.proactive_match_threshold` (nullable numeric).
- Global fallback from env/config default `0.60`.

Definition of Done (Story 3):
1. Only above-threshold matches generate notifications.
2. Threshold can vary per user.
3. Duplicate notification spam is prevented.

User Story 4: Opportunities UI Dashboard
As a job applicant,
I want a dedicated page of high-match proactive opportunities,
So I can review fit and choose where to apply.

Acceptance Criteria:
1. Create page `src/pages/ProactiveMatches.tsx`.
2. Add route and sidebar entry using existing app navigation patterns.
3. Query and display proactive matches for current user:
- Source: `sats_analyses` joined to system-generated proactive `sats_job_descriptions` (or dedicated proactive view).
- Filter: `match_score >= effective_threshold`.
- Sort: highest score first, newest first as secondary.
4. Card UI requirements:
- ATS score
- Missing skills
- Score breakdown (if available)
- Original external URL (`source_url`)
- Timestamp and source label

Definition of Done (Story 4):
1. User sees only their proactive high-match opportunities.
2. Cards link out to original posting.
3. UI is consistent with existing SmartATS page/component patterns.

Delivery sequencing and checkpoints:
1. Story 1: schema + fetch worker + cron + verification query.
2. Story 2: scoring worker + ATS persistence + idempotency checks.
3. Story 3: threshold hierarchy + notification persistence + dedupe.
4. Story 4: page + data query + UX polish.

Non-goals for initial P14 slice:
1. Auto-apply submission to external platforms.
2. Recursive crawler behavior.
3. Full multi-source marketplace optimization (keep mock/provider abstraction first).
